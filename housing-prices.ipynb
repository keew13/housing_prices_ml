{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Notebooks taken for reference:\n* https://www.kaggle.com/angqx95/data-science-workflow-top-2-with-tuning\n* https://www.kaggle.com/cheesu/house-prices-1st-approach-to-data-science-process\n* https://www.kaggle.com/datafan07/top-1-approach-eda-new-models-and-stacking#Model-Results\n* https://www.kaggle.com/itslek/stack-blend-lrs-xgb-lgb-house-prices-k-v17"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#importing the required libraries for...\n#handling and visualizing the data...\n#and creating Machine Learning Models...\n\nimport pandas as pd\nimport numpy as np\npd.set_option('display.max_columns', None)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn.linear_model import Ridge\nfrom xgboost import XGBRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#importing the data\n\ntrain = pd.read_csv(\"/kaggle/input/home-data-for-ml-course/train.csv\", index_col = 0, keep_default_na=False)\ntest = pd.read_csv(\"/kaggle/input/home-data-for-ml-course/test.csv\", index_col = 0, keep_default_na=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note: Pandas treats NA as NaN by default so explicitly it has been mentioned while reading the CSV file because some of our categorical features have NA as a separate class with meaning."},{"metadata":{"trusted":true},"cell_type":"code","source":"#checking the data and format first-hand\n\nprint(train.shape)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#checking the data and format first-hand\n\nprint(test.shape)\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A quick analysis on a rough notebook made us realize that certain numerical features had missing values denoted by NA and hence now we need to make them disappear from training.<br>\n<br>\nAdditionally certain categorical features that have NA as a separate class also had missing values represented by NA that now went unnoticed. So we find them explicity and make them disappear from both the sets.<br>\n<br>\nApart from this certain categorical features that do not have NA as a separate class contain values that we will have to remove.<br>\n<br>\nInitially we will only remove the values from training set and then later we will work and process on test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"#LotFrontage column is supposed to be purely numerical but...\nprint(train.loc[:, ['LotFrontage']].info())\ntrain.loc[train['LotFrontage']=='NA', ['LotFrontage']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So LotFrontage apparently has 259 missing values."},{"metadata":{"trusted":true},"cell_type":"code","source":"#another purely numerical feature that has NA is MasVnrArea\nprint(train.loc[:, ['MasVnrArea']].info())\ntrain.loc[train['MasVnrArea']=='NA', ['MasVnrArea']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"MasVnrArea therefore has 8 missing values."},{"metadata":{"trusted":true},"cell_type":"code","source":"#and the last numerical feature that has NA is GarageYrBlt\nprint(train.loc[:, ['GarageYrBlt']].info())\ntrain.loc[train['GarageYrBlt']=='NA', ['GarageYrBlt']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"GarageYrBlt has apparently 81 missing values."},{"metadata":{"trusted":true},"cell_type":"code","source":"#a categorical feature that cannot have NA value but possess in our dataset is Electrical\ntrain.loc[train['Electrical']=='NA', ['Electrical']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Electrical therefore has 1 missing value."},{"metadata":{"trusted":true},"cell_type":"code","source":"#contardictory values in these two fields of MasVnrArea and MasVnrType\ntrain.loc[((train['MasVnrArea']=='0') & (train['MasVnrType']!='None')) | ((train['MasVnrArea']!='0') & (train['MasVnrType']=='None')), ['MasVnrArea', 'MasVnrType']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we can consider the MasVnrArea as missing somewhere and 0 somewhere...<br>\nMissing Values in MasVnrArea(Id): 689, 1242<br>\nMissing Values in MasVnrType(Id): 625, 1301, 1335<br>\n0 values in MasVnrArea(Id): 774, 1231<br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#column of MasVnrType also has certain missing values because\ntrain.loc[train['MasVnrType']=='NA',['MasVnrType']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Oof...MasVnrType has 8 missing values."},{"metadata":{"trusted":true},"cell_type":"code","source":"#one of the categorical feature that can have NA value but\n#contradicts some other feature is BsmtExposure\n\ntrain.loc[(train['TotalBsmtSF']>0) & (train['BsmtExposure']=='NA'), ['TotalBsmtSF', 'BsmtExposure']]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"That's one missing value for sure."},{"metadata":{"trusted":true},"cell_type":"code","source":"#similarly\n\ntrain.loc[(train['TotalBsmtSF']>0) & (train['BsmtFinType2']=='NA'), ['TotalBsmtSF', 'BsmtFinType2']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Another missing value. Duh!"},{"metadata":{},"cell_type":"markdown","source":"I realized later that we should rather check for missing values in test set at this point only. We could have efficiently loop over with train and test set simultaneously but meh... "},{"metadata":{"trusted":true},"cell_type":"code","source":"#numerical feature LotFrotage coming in first\nprint(test.loc[:, ['LotFrontage']].info())\ntest.loc[test['LotFrontage']=='NA', ['LotFrontage']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"227 missing rows! duh"},{"metadata":{"trusted":true},"cell_type":"code","source":"#another numerical feature\nprint(test.loc[:, ['MasVnrArea']].info())\ntest.loc[test['MasVnrArea']=='NA', ['MasVnrArea']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"15 missing values!"},{"metadata":{"trusted":true},"cell_type":"code","source":"#another one\nprint(test.loc[:, ['TotalBsmtSF']].info())\ntest.loc[test['TotalBsmtSF']=='NA', ['TotalBsmtSF']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1 missing value"},{"metadata":{"trusted":true},"cell_type":"code","source":"#let's just club some features and display them\ntest.loc[:, ['BsmtUnfSF', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtFullBath', 'BsmtHalfBath']].info()\ntest.loc[(test['BsmtUnfSF']=='NA') | (test['BsmtFinSF1']=='NA') | (test['BsmtFinSF2']=='NA') | (test['BsmtFullBath']=='NA') |\n         (test['HalfBath']=='NA'), ['BsmtUnfSF', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtFullBath', 'BsmtHalfBath']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Missing and explicit 0s needed."},{"metadata":{"trusted":true},"cell_type":"code","source":"#ah missing...missing...actually not missing but the area is supposed to be 0 based on other columns\nprint(test.loc[:, ['GarageArea']].info())\ntest.loc[test['GarageArea']=='NA', ['GarageArea']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1 value needed."},{"metadata":{"trusted":true},"cell_type":"code","source":"#GarageCars as well\nprint(test.loc[:, ['GarageCars']].info())\ntest.loc[test['GarageCars']=='NA', ['GarageCars']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1 value needed."},{"metadata":{"trusted":true},"cell_type":"code","source":"#and the last feature of numerical type is\nprint(test.loc[:, ['GarageYrBlt']].info())\ntest.loc[test['GarageYrBlt']=='NA', ['GarageYrBlt']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This feature has 78 missing values. Turns out test set has more missing values than training set. lmao"},{"metadata":{"trusted":true},"cell_type":"code","source":"#MasVnrType doesn't have a class NA and still\nprint(test.loc[:, ['MasVnrType']].info())\ntest.loc[test['MasVnrType']=='NA', ['MasVnrType']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"16 missing values. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Utilities doesn't have a class NA and still\nprint(test.loc[:, ['Utilities']].info())\ntest.loc[test['Utilities']=='NA', ['Utilities']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2 missing values."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Utilities doesn't have a class NA and still\nprint(test.loc[:, ['MSZoning']].info())\ntest.loc[test['MSZoning']=='NA', ['MSZoning']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#contradictory columns values also need to be checked\ntest.loc[(test['TotalBsmtSF']!='0') & (test['BsmtExposure']=='NA'), ['TotalBsmtSF', 'BsmtExposure']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"aha! 2 values that contradict... and 1 that we need to figure out how to handle"},{"metadata":{"trusted":true},"cell_type":"code","source":"#contradictory columns values also need to be checked\ntest.loc[(test['TotalBsmtSF']!='0') & (test['BsmtQual']=='NA'), ['TotalBsmtSF', 'BsmtQual']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2 more. eh this is boring thing to do."},{"metadata":{"trusted":true},"cell_type":"code","source":"#contradictory columns values also need to be checked\ntest.loc[(test['TotalBsmtSF']!='0') & (test['BsmtCond']=='NA'), ['TotalBsmtSF', 'BsmtCond']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3 more values!"},{"metadata":{},"cell_type":"markdown","source":"this is a tricky one"},{"metadata":{"trusted":true},"cell_type":"code","source":"#contardictory values in these two fields of MasVnrArea and MasVnrType\ntest.loc[((test['MasVnrArea']=='0') & (test['MasVnrType']!='None')) | ((test['MasVnrArea']!='0') & (test['MasVnrType']=='None')), ['MasVnrArea', 'MasVnrType']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Missing Value in MasVnrType(Id): 1670<br>\nMissing Value in MasVnrArea(Id): 2320<br>\n0 value in MasVnrArea(Id): 2453<br>"},{"metadata":{},"cell_type":"markdown","source":"### Categorization of Features on basis of the type of the Variable they belong to\n\n* Nominal Values: MSSubClass, MSZoning, LandContour, Utilities, LotConfig, Neighborhood, Condition1, Condition2, BldgType, HouseStyle, RoofStyle, RoofMatl, Exterior1st, Exterior2nd, MasVnrType, Foundation, Heating, CentralAir, GarageType, MiscFeature, MoSold, SaleType, SaleCondition\n* Continuos: LotFrontage, LotArea, YearBuilt, YearRemodAdd, MasVnrArea, BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, 1stFlrSF, 2ndFlrSF, LowQualFinSF, GrLivArea, GarageArea, GarageYrBlt, WoodDeckSF, OpenPorchSF, EnclosedPorch, 3SsnPorch, ScreenPorch, PoolArea, MiscVal\n* Discrete: BsmtFullBath, BsmtHalfBath, FullBath, HalfBath, Bedroom, Kitchen, TotRmsAbvGrd, Fireplaces, GarageCars, YrSold \n* Ordinal: Street, Alley, LotShape, LandSlope, OverallQual, OverallCond, ExterQual, ExterCond, BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinType2, HeatingQC, Electrical, KitchenQual, Functional, FireplaceQu, GarageFinish, GarageQual, GarageCond, PavedDrive, PoolQC, Fence"},{"metadata":{},"cell_type":"markdown","source":"Before we begin with discriptive analysis let us just remove the NA values that are actually missing in the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"#handling the numerical columns first\nfor dataset in [train, test]:\n    dataset.loc[:, ['LotFrontage', 'MasVnrArea', 'GarageYrBlt']] = dataset.loc[:, ['LotFrontage', 'MasVnrArea', 'GarageYrBlt']].replace(to_replace =\"NA\", \n                     value = np.nan)\n    dataset.loc[:, ['LotFrontage', 'MasVnrArea', 'GarageYrBlt']] = dataset.loc[:, ['LotFrontage', 'MasVnrArea', 'GarageYrBlt']].astype(float)\n    print('*'*50)\n    print(dataset.loc[:, ['LotFrontage', 'MasVnrArea', 'GarageYrBlt']].info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#let us now explicitly remove the NA value from Electrical\nfor dataset in [train, test]:\n    dataset.loc[dataset['Electrical']=='NA', 'Electrical'] = np.nan\n    print('*'*50)\n    print(dataset.loc[:, ['Electrical']].info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#now let's remove or edit values from MasVnrArea and MasVnrType\ntrain.loc[[689, 1242], 'MasVnrArea'] = np.nan\ntest.loc[[2320], 'MasVnrArea'] = np.nan\ntrain.loc[[625, 1301, 1335], 'MasVnrType'] = np.nan\ntest.loc[[1670], 'MasVnrType'] = np.nan\ntrain.loc[[774, 1231], 'MasVnrArea'] = 0\ntest.loc[[2453], 'MasVnrArea'] = 0\ntrain.loc[:, ['MasVnrArea']] = train.loc[:, ['MasVnrArea']].astype(float)\ntest.loc[:, ['MasVnrArea']] = test.loc[:, ['MasVnrArea']].astype(float)\nprint(train.loc[:, ['MasVnrArea', 'MasVnrType']].info())\nprint('*'*50)\nprint(test.loc[:, ['MasVnrArea', 'MasVnrType']].info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#now let's remove NA values from MasVnrType\nfor dataset in [train, test]:\n    dataset.loc[:, ['MasVnrType']] = dataset.loc[:, ['MasVnrType']].replace(to_replace =\"NA\", value = np.nan)\n    print('*'*50)\n    print(dataset.loc[:, ['MasVnrType']].info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#removing NA values from Utilities\ntest.loc[[1916, 1946], 'Utilities'] = np.nan","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#removing NA values from MSZoning\ntest.loc[[1916, 2217, 2251, 2905], ['MSZoning']] = np.nan","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#BsmtExposure here we come\ntrain.loc[949, 'BsmtExposure'] = np.nan\ntest.loc[[1488, 2349], 'BsmtExposure'] = np.nan\nprint(train.loc[:, ['BsmtExposure']].info())\nprint('*'*50)\nprint(test.loc[:, ['BsmtExposure']].info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Bsmt related data in test\ntest.loc[2121, ['BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF']] = [0, 0, 0, 0]\ntest.loc[:, ['BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF']] = test.loc[:, ['BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF']].astype(float)\ntest.loc[2121, ['BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Bsmt related data in test\ntest.loc[[2121, 2189], ['BsmtFullBath', 'BsmtHalfBath']] = [[0, 0], [0, 0]]\ntest.loc[:, ['BsmtFullBath', 'BsmtHalfBath']] = test.loc[:, ['BsmtFullBath', 'BsmtHalfBath']].astype(float)\ntest.loc[[2121, 2189], ['BsmtFullBath', 'BsmtHalfBath']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Bsmt related data in test\ntest.loc[[2218, 2219], ['BsmtQual']] = [[np.nan], [np.nan]]\ntest.loc[:, ['BsmtQual']].info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Bsmt related data in test\ntest.loc[[2041, 2186, 2525], ['BsmtCond']] = [[np.nan], [np.nan], [np.nan]]\ntest.loc[:, ['BsmtCond']].info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#0s in Garage related data\ntest.loc[2577, ['GarageArea', 'GarageCars']] = [0, 0]\ntest.loc[:, ['GarageArea', 'GarageCars']] = test.loc[:, ['GarageArea', 'GarageCars']].astype(float)\ntest.loc[2577, ['GarageArea', 'GarageCars']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#BsmtFinType2 and then we are done\ntrain.loc[333, 'BsmtFinType2'] = np.nan\ntrain.loc[:, ['BsmtFinType2']].info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can peacefully begin with analysis of other data!<br>\nStatistically starting would be a good thing!"},{"metadata":{"trusted":true},"cell_type":"code","source":"#statistical description\nprint(train.describe().shape)\ntrain.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* In total 37 columns have numerical values including Id and few others which need certain transformation.\n* Some columns might have outliers that need to be checked.\n* Few columns have missing values as we have already talked about that.\n* The last entry of a house being built or remodelled was recorded in the year 2010.\n* Overall condition and quality on average seem to be around 5-6.\n* A lot of houses do not have fancy or redundant factors like having unnecessary amount of baths or a wood deck. We might want to remove such features that mostly have a single value or class."},{"metadata":{"trusted":true},"cell_type":"code","source":"#description of categorical or other attributes\nprint(train.describe(include='O').shape)\ntrain.describe(include='O')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* There are 43 columns with non-numerical data. Some of these might have ordinal values. We need to check.\n* Few columns have missing values but the count is less. Let's see what we can do about that.\n* Most of the houses have same classfier values.\n* As the numerical ordinal ratings for house and other features were on average average, same is the case here."},{"metadata":{"trusted":true},"cell_type":"code","source":"#finding the insights into missing data\nprint(train.info())\nprint('*'*50)\nprint(test.info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#getting the count idea of missing values in train set\nmissing = {}\nfor i in range(0, len(train.columns)):\n    if train.loc[:, [train.columns[i]]].isnull().sum()[train.columns[i]]>0:\n        missing[train.columns[i]] = train.loc[:, [train.columns[i]]].isnull().sum()[train.columns[i]]\nmissing = pd.DataFrame([[i, j] for i, j in zip(list(missing.keys()), list(missing.values()))], columns=['Column', '#missing'])\nmissing","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#getting the count idea of missing values in test set\nmissing = {}\nfor i in range(0, len(test.columns)):\n    if test.loc[:, [test.columns[i]]].isnull().sum()[test.columns[i]]>0:\n        missing[test.columns[i]] = test.loc[:, [test.columns[i]]].isnull().sum()[test.columns[i]]\nmissing = pd.DataFrame([[i, j] for i, j in zip(list(missing.keys()), list(missing.values()))], columns=['Column', '#missing'])\nmissing","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some columns with very less missing values can be dropped or replaced with the most frequent value(categorical and discrete).<br>\nWe can also think for other techniques. For example houses in the same locality or having similar features can be used to find missing values. In fact that is the gist in KNN imputing technqiue. NeIgHbOrS!\nSome other techniques can also be used which we can discuss later."},{"metadata":{},"cell_type":"markdown","source":"Now we aim to analyze the data even more. We will use viusalization tools to our aid.<br>\nWe shall first separate out the columns containing numerical values and those containing objects (classes of some kind)."},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating separate numerical and non-numerical dataframe for analysis\n#a quick analysis in data description tells us that MSSubClass is actually a categorical column with\n#numerical values\nnumerical = train.select_dtypes(exclude=['object']).drop(['MSSubClass'], axis=1).copy()\nnumerical.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating separate numerical and non-numerical dataframe for analysis\ncategorical = train.select_dtypes(include=['object']).copy()\ncategorical['MSSubClass'] = train['MSSubClass']\ncategorical.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First we will perform Univariate analysis by checking the distribution and various other plots."},{"metadata":{"trusted":true},"cell_type":"code","source":"#let us first check the skewness of the target variable\n\n#graph = sns.FacetGrid(train, height=4, aspect=1.33)\n#graph.map(sns.distplot, 'SalePrice')\nfig, axes = plt.subplots(figsize=(12, 6))\nsns.distplot(train['SalePrice'])\nprint(train['SalePrice'].skew())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Obviously the data is skewed with most of the data lying towards the left region where SalePrice is between 100000 and 300000. The target variable is positive skewed. A good thing to do would be to convert it to a Normal distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"#we apply log function to the target variable\n\n#graph = sns.FacetGrid(train, height=4, aspect=1.33)\n#graph.map(sns.distplot, 'SalePrice')\nfig, axes = plt.subplots(figsize=(12, 6))\nsns.distplot(np.log(train['SalePrice']))\nprint(np.log(train['SalePrice']).skew())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"That looks like a nice distribution. Fairly symmetric. GG<br>\nWe need to log-transform the target variable in the preprocessing stage."},{"metadata":{"trusted":true},"cell_type":"code","source":"#let us check if the independent variables also are skewed or...\n#if they need any transformation\n\nfig, ax = plt.subplots(figsize=(12, 18))\nfor i in range(0, len(numerical.columns)):\n    fig.add_subplot(9,4,i+1)\n    plt.title(str(numerical.iloc[:, i].skew()))\n    sns.distplot(numerical.iloc[:, i], kde=False)\nfig.tight_layout(pad=1.0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Some features have discrete values.\n* The ones with continuous values are skewed.\n* Many features have one value (0) that over-dominates the other values."},{"metadata":{"trusted":true},"cell_type":"code","source":"#let us go for statistical inference\n#boxplots will help us for the same and help in detecting the presence of outliers if any\n#it will be better if we only plot for continuos variables\ncont = ['LotFrontage', 'LotArea', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', \n        'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'GarageArea',\n        'GarageYrBlt', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal']\n\nfig, ax = plt.subplots(figsize=(12, 18))\nfor i in range(0, len(cont)):\n    fig.add_subplot(6,4,i+1)\n    sns.boxplot(y = cont[i], data = numerical)\nfig.tight_layout(pad=1.0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Because some continuous variables have the same value (mostly 0), the box plot will tend to find outliers.<br>\nOther than that some other features also seem to contain certain outliers.<br>\nWe might have to decide a threshold for each of them."},{"metadata":{"trusted":true},"cell_type":"code","source":"#now let's get a count idea of categorical ones visually\n\nfig, ax = plt.subplots(figsize=(12,20))\nfor i in range(len(categorical.columns)):\n    fig.add_subplot(9,5,i+1)\n    sns.countplot(x=categorical.iloc[:,i], data=categorical.dropna())\n    plt.xticks(rotation=90)\nfig.tight_layout(pad=1.0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In some of the features we see that one of the class over-dominates the other classes by a large factor.<br>\nGotta check that later."},{"metadata":{},"cell_type":"markdown","source":"Now to get an idea of relationships between features with themselves and target variable let us begin with Bivariate analysis.<br>\nWe begin with numerical features first."},{"metadata":{"trusted":true},"cell_type":"code","source":"#first let's get the idea of correlation between numerical features\n\nfig, ax = plt.subplots(figsize=(12, 8))\ncorrelation = numerical.corr()\nsns.heatmap(correlation, mask=correlation<0.8, linewidth=0.5, cmap='Blues')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Clearly some of these features are strongly related and we surely do not want redundancy.\nThere are four pairs of such strong correlation that we seek to remove later.\nThese are:\n* 1stFlrSF and TotalBsmtSF\n* GarageYrBlt and YearBuilt\n* TotRmsAbvGrd and GrLivArea\n* GarageCars and GarageArea"},{"metadata":{"trusted":true},"cell_type":"code","source":"#let us now get an idea of how other numeric features contribute in predicting the SalePrice\n#by the measure of their correlation with it\n\ncorrelation = numerical.corr()\ncorrelation[['SalePrice']].sort_values(['SalePrice'],ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Obviously it is evident that some numerical features can help us in predicting the SalePrice strongly.<br>\nPeople indeed are concerned with the OverallQual, GrLivArea, GarageArea and many other features that are shown in the above DataFrame."},{"metadata":{"trusted":true},"cell_type":"code","source":"#lastly let us just do a scatterplot analysis to visualize the linear relationship\n#between numerical and target feature\n\nfig, ax = plt.subplots(figsize=(12, 18))\nfor i in range(0, len(numerical.columns)):\n    fig.add_subplot(9,4,i+1)\n    sns.scatterplot(x = numerical.columns[i], y = 'SalePrice', data=numerical)\nfig.tight_layout(pad=1.0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There's certainly some linearity, but not that much that it would be worth finding a quantitative measure for them.<br>\n<br>\nLet us now move towards preprocessing the data from our finding.<br>\nWe shall be performing:\n* Finding missing values\n* Removing outliers\n* Encoding the features correctly for both nominal and ordinal categories\n* Creating bands/bins of data if necessary\n* OneHotEncoding or creating the dummy columns for nominal features\n* Removing redundant columns\n* Generating new columns if necessary\n<br><br>\n*let's go* **pubgnoises**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#first we remove the redundant numerical features that we found have high correlation\n#we shall remove the one from the pair that has comparitvely less correlation with the\n#target variable\ntrain = train.drop(['1stFlrSF', 'GarageYrBlt', 'TotRmsAbvGrd', 'GarageArea'], axis=1)\ntest = test.drop(['1stFlrSF', 'GarageYrBlt', 'TotRmsAbvGrd', 'GarageArea'], axis=1)    \nprint(train.info())\nprint('*'*50)\nprint(test.info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#removing categorical columns with a lot of same values (96%)\ncat_col = train.select_dtypes(include=['object']).columns\noverfit_cat = []\nfor i in cat_col:\n    counts = train[i].value_counts()\n    zeros = counts.iloc[0]\n    if zeros / len(train[i]) * 100 > 96:\n        overfit_cat.append(i)\nprint(overfit_cat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#removing numericalcal columns with a lot of same values (96%)\nnum_col = train.select_dtypes(exclude=['object']).columns\noverfit_num = []\nfor i in num_col:\n    counts = train[i].value_counts()\n    zeros = counts.iloc[0]\n    if zeros / len(train[i]) * 100 > 96:\n        overfit_num.append(i)\nprint(overfit_num)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.drop(overfit_cat, axis=1)\ntrain = train.drop(overfit_num, axis=1)\ntest = test.drop(overfit_cat, axis=1)\ntest = test.drop(overfit_num, axis=1)\nprint(train.info())\nprint('*'*50)\nprint(test.info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#removing outliers\ntrain = train.drop(train[train['LotArea'] > 100000].index)\ntrain.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#locating missing values in MasVnrArea and MasVnrType\nprint('Train Set')\nprint(train.loc[train['MasVnrType'].isnull(), ['MasVnrArea', 'MasVnrType']])\nprint('Test Set')\nprint(test.loc[test['MasVnrType'].isnull(), ['MasVnrArea', 'MasVnrType']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#now we replace the missing values in MasVnrType with BrkFace\n#for those which have certain MasVnrArea associated with them\ntrain.loc[[625, 1301, 1335], ['MasVnrType']] = ['BrkFace' for _ in range(0, 3)]\ntest.loc[[1670, 2611], ['MasVnrType']] = ['BrkFace', 'BrkFace']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#replacing MasVnrArea with 0 and MasVnrType with None\n#when none of the information is provided about them together\ntrain.loc[train['MasVnrType'].isnull(), ['MasVnrArea', 'MasVnrType']] = [[0, 'None']]*len(train.loc[train['MasVnrType'].isnull(), ['MasVnrArea', 'MasVnrType']])\ntest.loc[test['MasVnrType'].isnull(), ['MasVnrArea', 'MasVnrType']] = [[0, 'None']]*len(test.loc[test['MasVnrType'].isnull(), ['MasVnrArea', 'MasVnrType']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#replacing BsmtExposure with No\ntrain.loc[train['BsmtExposure'].isnull(), ['BsmtExposure']] = ['No']*len(train.loc[train['BsmtExposure'].isnull(), ['BsmtExposure']])\ntest.loc[test['BsmtExposure'].isnull(), ['BsmtExposure']] = ['No']*len(test.loc[test['BsmtExposure'].isnull(), ['BsmtExposure']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#replacing BsmtFinType2 with Rec\ntrain.loc[train['BsmtFinType2'].isnull(), ['BsmtFinType2']] = ['Rec']*len(train.loc[train['BsmtFinType2'].isnull(), ['BsmtFinType2']])\ntrain.loc[train['BsmtFinType2'].isnull(), ['BsmtFinType2']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#replacing BsmtQual, BsmtCond with TA\ntest.loc[test['BsmtQual'].isnull(), ['BsmtQual']] = ['TA']*len(test.loc[test['BsmtQual'].isnull(), ['BsmtQual']])\ntest.loc[test['BsmtCond'].isnull(), ['BsmtCond']] = ['TA']*len(test.loc[test['BsmtCond'].isnull(), ['BsmtCond']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#replacing Electrical with SBrkr\ntrain.loc[train['Electrical'].isnull(), ['Electrical']] = ['SBrkr']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#replacing MSZoning with RL\ntest.loc[test['MSZoning'].isnull(), ['MSZoning']] = ['RL']*len(test.loc[test['MSZoning'].isnull(), ['MSZoning']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating a merged dataset to make our job of encoding and finidng missing values easy\nmerged_df = pd.concat([train.iloc[:, :-1], test], axis=0)\nmerged_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating dummy variables\nmssubclass_dummies = pd.get_dummies(merged_df.MSSubClass, prefix=\"MSSubClass\")\nmszoning_dummies = pd.get_dummies(merged_df.MSZoning, prefix=\"MSZoning\")\nmerged_df.drop(['MSSubClass', 'MSZoning'], axis = 1, inplace = True)\nmerged_df = pd.concat([mssubclass_dummies.iloc[:, 1:], mszoning_dummies.iloc[:, 1:], merged_df], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating dummy variables\nlandcontour_dummies = pd.get_dummies(merged_df.LandContour, prefix=\"LandContour\")\nlotconfig_dummies = pd.get_dummies(merged_df.LotConfig, prefix=\"LotConfig\")\nmerged_df = pd.concat([merged_df.loc[:, :'LandContour'], landcontour_dummies.iloc[:, 1:],\n                       merged_df.loc[:, ['LandContour']], lotconfig_dummies.iloc[:, 1:], merged_df.loc[:, 'LotConfig':]], axis = 1)\nmerged_df.drop(['LandContour', 'LotConfig'], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating dummy variables\nneighborhood_dummies = pd.get_dummies(merged_df.Neighborhood, prefix=\"Neighborhood\")\ncondition1_dummies = pd.get_dummies(merged_df.Condition1, prefix=\"Condition1\")\nbldgtype_dummies = pd.get_dummies(merged_df.BldgType, prefix=\"BldgType\")\nhousestyle_dummies = pd.get_dummies(merged_df.HouseStyle, prefix=\"HouseStyle\")\nmerged_df = pd.concat([merged_df.loc[:, :'Neighborhood'], neighborhood_dummies.iloc[:, 1:],\n                       merged_df.loc[:, ['Neighborhood']], condition1_dummies.iloc[:, 1:],\n                       merged_df.loc[:, ['Condition1']], bldgtype_dummies.iloc[:, 1:],\n                       merged_df.loc[:, ['BldgType']], housestyle_dummies.iloc[:, 1:],\n                       merged_df.loc[:, 'HouseStyle':]], axis=1)\nmerged_df.drop(['Neighborhood', 'Condition1', 'BldgType', 'HouseStyle'], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating dummy variables\nroofstyle_dummies = pd.get_dummies(merged_df.RoofStyle, prefix=\"RoofStyle\")\nexterior1st_dummies = pd.get_dummies(merged_df.Exterior1st, prefix=\"Exterior1st\")\nexterior2nd_dummies = pd.get_dummies(merged_df.Exterior2nd, prefix=\"Exterior2nd\")\nmasvnrtype_dummies = pd.get_dummies(merged_df.MasVnrType, prefix=\"MasVnrType\")\nmerged_df = pd.concat([merged_df.loc[:, :'RoofStyle'], roofstyle_dummies.iloc[:, 1:],\n                       merged_df.loc[:, ['RoofStyle']], exterior1st_dummies.iloc[:, 1:],\n                       merged_df.loc[:, ['Exterior1st']], exterior2nd_dummies.iloc[:, 1:],\n                       merged_df.loc[:, ['Exterior2nd']], masvnrtype_dummies.iloc[:, 1:],\n                       merged_df.loc[:, 'MasVnrType':]], axis=1)\nmerged_df.drop(['RoofStyle', 'Exterior1st', 'Exterior2nd', 'MasVnrType'], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating dummy variables\nfoundation_dummies = pd.get_dummies(merged_df.Foundation, prefix=\"Foundation\")\nmerged_df = pd.concat([merged_df.loc[:, :'Foundation'], foundation_dummies.iloc[:, 1:],\n                       merged_df.loc[:, 'Foundation':]], axis=1)\nmerged_df.drop(['Foundation'], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating dummy variables\ncentralair_dummies = pd.get_dummies(merged_df.CentralAir, prefix=\"CentralAir\")\nmerged_df = pd.concat([merged_df.loc[:, :'CentralAir'], centralair_dummies.iloc[:, 1:],\n                       merged_df.loc[:, 'CentralAir':]], axis=1)\nmerged_df.drop(['CentralAir'], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating dummy variables\ngaragetype_dummies = pd.get_dummies(merged_df.GarageType, prefix=\"GarageType\")\nmerged_df = pd.concat([merged_df.loc[:, :'GarageType'], garagetype_dummies.iloc[:, 1:],\n                       merged_df.loc[:, 'GarageType':]], axis=1)\nmerged_df.drop(['GarageType'], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating dummy variables\nmosold_dummies = pd.get_dummies(merged_df.MoSold, prefix=\"MoSold\")\nmerged_df = pd.concat([merged_df.loc[:, :'MoSold'], mosold_dummies.iloc[:, 1:],\n                       merged_df.loc[:, 'MoSold':]], axis=1)\nmerged_df.drop(['MoSold'], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating dummy variables\nsaletype_dummies = pd.get_dummies(merged_df.SaleType, prefix=\"SaleType\")\nsalecondition_dummies = pd.get_dummies(merged_df.SaleCondition, prefix=\"SaleCondition\")\nmerged_df = pd.concat([merged_df.loc[:, :'SaleType'], saletype_dummies.iloc[:, 1:],\n                       merged_df.loc[:, ['SaleType']], salecondition_dummies.iloc[:, 1:],\n                       merged_df.loc[:, 'SaleCondition':]], axis=1)\nmerged_df.drop(['SaleType', 'SaleCondition'], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#mapping ordinal values\nordinal_map = {'Alley' : {'Grvl':2, 'Pave':1, 'NA':0},\n'LotShape' : {'Reg':3, 'IR1':2, 'IR2':1, 'IR3':0},\n'LandSlope' : {'Gtl':2, 'Mod':1, 'Sev':0},\n'ExterQual' : {'Ex':4, 'Gd':3, 'TA':2, 'Fa':1, 'Po':0},\n'ExterCond' : {'Ex':4, 'Gd':3, 'TA':2, 'Fa':1, 'Po':0},\n'BsmtQual' : {'Ex':5, 'Gd':4, 'TA':3, 'Fa':2, 'Po':1, 'NA':0},\n'BsmtCond' : {'Ex':5, 'Gd':4, 'TA':3, 'Fa':2, 'Po':1, 'NA':0},\n'BsmtExposure' : {'Gd':4, 'Av':3, 'Mn':2, 'No':1, 'NA':0},\n'BsmtFinType1' : {'GLQ':6, 'ALQ':5, 'BLQ':4, 'Rec':3, 'LwQ':2, 'Unf':1, 'NA':0},\n'BsmtFinType2' : {'GLQ':6, 'ALQ':5, 'BLQ':4, 'Rec':3, 'LwQ':2, 'Unf':1, 'NA':0},\n'HeatingQC' : {'Ex':4, 'Gd':3, 'TA':2, 'Fa':1, 'Po':0},\n'Electrical' : {'SBrkr':4, 'FuseA':3, 'FuseF':2, 'FuseP':1, 'Mix':0},\n'KitchenQual' : {'Ex':4, 'Gd':3, 'TA':2, 'Fa':1, 'Po':0},\n'Functional' : {'Typ':7, 'Min1':6, 'Min2':5, 'Mod':4, 'Maj1':3, 'Maj2':2, 'Sev':1, 'Sal':0},\n'FireplaceQu' : {'Ex':5, 'Gd':4, 'TA':3, 'Fa':2, 'Po':1, 'NA':0},\n'GarageFinish' : {'Fin':3, 'RFn':2, 'Unf':1, 'NA':0},\n'GarageQual' : {'Ex':5, 'Gd':4, 'TA':3, 'Fa':2, 'Po':1, 'NA':0},\n'GarageCond' : {'Ex':5, 'Gd':4, 'TA':3, 'Fa':2, 'Po':1, 'NA':0},\n'PavedDrive' : {'Y':2, 'P':1, 'N':0},\n'Fence' : {'GdPrv':4, 'MnPrv':3, 'GdWo':2, 'MnWw':1, 'NA':0}}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#mapping ordinal values\nfor col in list(ordinal_map.keys()):\n    merged_df[col] = merged_df[col].map(ordinal_map[col])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#separating the datasets again for ease at performing next steps\ntrain_X = merged_df.loc[1:1460, :].copy()\ntrain_y = train.iloc[:, -1].copy()\ntest_X = merged_df.loc[1461:, :].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#computing missing values through KNN technique\nfrom sklearn.impute import KNNImputer\nimputer = KNNImputer(n_neighbors=5)\nimputer.fit(train_X.iloc[:, :])\ntrain_X.iloc[:, :]=imputer.transform(train_X.iloc[:, :])\ntest_X.iloc[:, :]=imputer.transform(test_X.iloc[:, :])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#converting dtype for appropriate columns\ncols = ['LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '2ndFlrSF', \n        'LowQualFinSF', 'GrLivArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', 'ScreenPorch']\nfor i in train_X.columns:\n    if i not in cols:\n        train_X.loc[:, [i]] = train_X.loc[:, [i]].astype(int)\n        test_X.loc[:, [i]] = test_X.loc[:, [i]].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#applying log transformation to skewed variables\nskewed_cols = ['LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'TotalBsmtSF',\n              'GrLivArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', 'ScreenPorch']\nfor dataset in [train_X, test_X]:\n    for col in skewed_cols:\n        dataset[col] = np.log1p(dataset[col])\ntrain_y = np.log(train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating training and validation sets\nfrom sklearn.model_selection import train_test_split\ntrain_X = train_X.values\ntrain_y = train_y.values\ntest_X = test_X.values\nX_train, X_val, y_train, y_val = train_test_split(train_X, train_y, test_size=0.2, random_state=13)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating copies of the training and validation sets\n#and making them scaled for some other regression models\nfrom sklearn.preprocessing import StandardScaler\nX_train_scaled, X_val_scaled, y_train_scaled, y_val_scaled = X_train.copy(), X_val.copy(), y_train.copy(), y_val.copy()\ntest_X_scaled = test_X.copy()\nsc_X = StandardScaler()\nsc_X.fit(X_train_scaled)\nX_train_scaled = sc_X.transform(X_train_scaled)\nX_val_scaled = sc_X.transform(X_val_scaled)\ntest_X_scaled = sc_X.transform(test_X_scaled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#defining helper functions\nfrom sklearn.model_selection import cross_val_score\ndef trainer(model, X, y, test):\n    model.fit(X, y)\n    return model.predict(test), cross_val_score(estimator=model, X=X, y=y, cv=10, scoring = 'neg_mean_absolute_error')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Base Models\n#Ridge Regressor\nrid_reg = Ridge(random_state=13)\nrid_y, rid_score = trainer(rid_reg, X_train_scaled, y_train_scaled, X_val_scaled)\n\n#Support Vector Regressor\nsvr_reg = SVR()\nsvr_y, svr_score = trainer(svr_reg, X_train_scaled, y_train_scaled, X_val_scaled)\n\n#Random Forest Regressor\nrf_reg = RandomForestRegressor(random_state=13)\nrf_y, rf_score = trainer(rf_reg, X_train, y_train, X_val)\n\n#XGBoost Regressor\nxgb_reg = XGBRegressor()\nxgb_y, xgb_score = trainer(xgb_reg, X_train, y_train, X_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#comparing the score returned by cross_val_score\n#even though the scoring is done on log values of predictions\nscores = {'Model':['Ridge', 'SVR', 'Random Forest', 'XGBoost'], 'MAE': [rid_score.mean(), svr_score.mean(), rf_score.mean(), xgb_score.mean()],\n          'SD': [rid_score.std(), svr_score.std(), rf_score.std(), xgb_score.std()]}\nscores = pd.DataFrame([[i, -1*j, k] for i, j, k in zip(scores['Model'], scores['MAE'], scores['SD'])], columns=list(scores.keys()))\nscores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#now we inverse log the predictions returned to check MAE\ny_val = np.exp(y_val)\ny_val_scaled = np.exp(y_val_scaled)\nrid_y = np.exp(rid_y)\nsvr_y = np.exp(svr_y)\nrf_y = np.exp(rf_y)\nxgb_y = np.exp(xgb_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#checking the performance of our models based on MAE\nfrom sklearn.metrics import mean_absolute_error\nrid_val_score = mean_absolute_error(y_val_scaled, rid_y)\nsvr_val_score = mean_absolute_error(y_val_scaled, svr_y)\nrf_val_score = mean_absolute_error(y_val, rf_y)\nxgb_val_score = mean_absolute_error(y_val, xgb_y)\nval_scores = pd.DataFrame([['Ridge', rid_val_score],\n                           ['SVR', svr_val_score],\n                           ['Random Forest', rf_val_score],\n                           ['XGBoost', xgb_val_score]], columns=['Model', 'MAE'])\nval_scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#now we blend the models and give vote weight\n#on basis of their MAE on validation set\nblend_y = (0.35*rid_y) + (0.1*svr_y) + (0.2*rf_y) + (0.35*xgb_y)\nblend_score = mean_absolute_error(y_val, blend_y)\nblend_score = pd.DataFrame([['Blend Model', blend_score]], columns = ['Model', 'MAE'])\nblend_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#predicting the test set now\n#Base Models\n#Ridge Regressor\nrid_reg = Ridge(random_state=13)\nrid_y, rid_score = trainer(rid_reg, X_train_scaled, y_train_scaled, test_X_scaled)\n\n#Support Vector Regressor\nsvr_reg = SVR()\nsvr_y, svr_score = trainer(svr_reg, X_train_scaled, y_train_scaled, test_X_scaled)\n\n#Random Forest Regressor\nrf_reg = RandomForestRegressor(random_state=13)\nrf_y, rf_score = trainer(rf_reg, X_train, y_train, test_X)\n\n#XGBoost Regressor\nxgb_reg = XGBRegressor()\nxgb_y, xgb_score = trainer(xgb_reg, X_train, y_train, test_X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#now we inverse log the predictions and blend them\nrid_y = np.exp(rid_y)\nsvr_y = np.exp(svr_y)\nrf_y = np.exp(rf_y)\nxgb_y = np.exp(xgb_y)\nblend_y = (0.35*rid_y) + (0.1*svr_y) + (0.2*rf_y) + (0.35*xgb_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating the submission file\nsubmission = pd.DataFrame({'Id':pd.read_csv(\"/kaggle/input/home-data-for-ml-course/test.csv\", keep_default_na=False)['Id'], 'SalePrice':blend_y})\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv(\"Housing_Submission.csv\", index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}